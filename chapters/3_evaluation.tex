\chapter{Quality of experience evaluation}
\label{cha:eval}

In this section, we illustrate how we set up a \textbf{testbed} with ComNetsEmu to evaluate the performance of \textbf{low-latency adaptive bitrate live streaming} and the results that we obtained. Some important metrics are extracted and plotted to highlight the issues of current streaming technologies in low-latency scenarios. The results of the evaluation will be the basis for some proposed improvements that we believe produce an overall better \textbf{quality of experience}.


\section{Testbed setup}
\label{sec:eval/testbed}

The testbed for evaluating the quality of experience (QoE) of existing live streaming solutions, such as HLS and DASH, was built with ComNetsEmu. The project was published as an open source project at \url{https://github.com/matteocontrini/live-streaming-testbed}.

The general architecture of the testbed is shown in Figure X. The topology of the emulated network consists of three hosts and two switches. The link between the two switches (S1 and S2) represents an unreliable network such as an Internet access network. When the emulation is run, a series of experiments with different configurations are run, and the link parameters are changed. For example, the bandwidth is varied to emulate a real network with oscillating bandwidth.

The three hosts contain Docker applications for a live streaming setup. More in detail:

\begin{itemize}
    \item \texttt{H3} hosts a script that performs the \textbf{packaging of a live stream} in both HLS and DASH formats. It runs an \ffmpeg{} command that simulates a live stream, taking an MP4 file as input. The output of this application, called \textit{live source}, is manifest and playlist files for both HLS and DASH, and the corresponding video and audio segments.
    \item \texttt{H2} acts as an \textbf{HTTP CDN server} for static files. It serves the files produced by the live source over HTTP/1.1, HTTP/2 and HTTP/3 through the \texttt{h2o} web server. It also takes care of provisioning the public key certificates that are required for HTTPS.
    \item \texttt{H1}, on the other side of the main network link, is the client. It consists of a frontend application containing a \textbf{video player} that plays the live video stream generated by the server. The frontend application also takes care of collecting several metrics from the player and transmit them to a Node.js backend application. The frontend application runs in a \textbf{Chromium headless} instance, which is started by the backend application through the Puppeteer library.
\end{itemize}

% TODO: diagram

To start the emulation through ComNetsEmu, one first needs to clone the testbed source code and prepare the MP4 file for the live source, as we will see in detail in the next section. Then, a Vagrant session should be started:

\begin{minted}[frame=single]{bash}
vagrant up
vagrant ssh
\end{minted}

Finally, after building the Docker images, the emulation can be started. The project source code contains scripts to run these commands more easily.

\begin{minted}[frame=single]{bash}
cd live-source && docker build -t live-source .
cd ../cdn && docker build -t cdn .
cd ../client && docker build -t client .
cd .. && sudo python3 topology.py
\end{minted}

Before going into the details of how the specific components were developed, it should be noted that the default Vagrant configuration for ComNetsEmu (contained in a file known as \texttt{Vagrantfile}) does not work when the machine architecture is ARM. This is especially problematic on newer laptops that use Apple Silicon hardware, such as the M1 and M2 chips.

To solve this problem, two changes must be applied to ComNetsEmu's \texttt{Vagrantfile}. First, the Vagrant box (the operating system image) must be changed to an image that is built for ARM architectures. Then, since x86 virtualization software like VirtualBox cannot be used on ARM systems, a new virtual machine provider must be added. In practice, for Mac systems, this means enabling Parallels Desktop as a virtual machine provider. Figure \ref{fig:vagrantfile} shows how the \texttt{Vagrantfile} of ComNetsEmu was changed.

\begin{figure}
    \centering
    \begin{minted}[frame=single,linenos,fontsize=\small]{diff}
diff --git a/Vagrantfile b/Vagrantfile
index 0f07076..a21824f 100644
--- a/Vagrantfile
+++ b/Vagrantfile
@@ -20,6 +20,8 @@ VM_NAME = "ubuntu-20.04-comnetsemu"
 # When using libvirt as the provider, use this box, bento boxes do not support...
 BOX_LIBVIRT = "generic/ubuntu2004"

+BOX_PARALLELS = "jeffnoxon/ubuntu-20.04-arm64"
+
 ######################
 #  Provision Script  #
 ######################
@@ -105,6 +107,14 @@ Vagrant.configure("2") do |config|
     # Sync ./ to home directory of vagrant to simplify the install script
     comnetsemu.vm.synced_folder ".", "/vagrant", disabled: true
     comnetsemu.vm.synced_folder ".", "/home/vagrant/comnetsemu"
+
+    # Parallels Desktop
+    config.vm.provider "parallels" do |prl, override|
+      override.vm.box = BOX_PARALLELS
+      prl.name = VM_NAME
+      prl.cpus = CPUS
+      prl.memory = RAM
+    end

     # For Virtualbox provider
     comnetsemu.vm.provider "virtualbox" do |vb|
    \end{minted}
    \caption{Caption}
    \label{fig:vagrantfile}
\end{figure}


\subsection{Live stream generation and packaging}
\label{sec:eval/testbed/packaging}

There are multiple ways to simulate a live video stream without having an actual live video source such as a camera. Although there exist many streaming servers that handle video ingestion, transcoding and packaging, we chose to implement a simpler solution based on \ffmpeg{} and a static input video file.

Moreover, since the source of the live stream is not dynamic, we decided to encode the video and audio renditions beforehand. The renditions are then packaged as live streams each time the emulation is run, as if they were ingested and transcoded live. This solution makes the test setup much lighter in terms of required computational resources.

When encoding the video renditions, we chose the following bitrate ladder. All video renditions are encoded with H.264 at 25 fps with a keyframe/I-frame interval of 2 seconds and a constant bitrate.

\begin{itemize}
    \item \texttt{1280x720} at 3.5 Mbps;
    \item \texttt{960x540} at 2.5 Mbps;
    \item \texttt{640x360} at 1.5 Mbps;
    \item \texttt{480x720} at 0.8 Mbps.
\end{itemize}

The audio is instead encoded with \texttt{AAC-LC} at 128 kbps. There are of course many more possible configurations, but the above is a common configuration that gives good quality results.\cite{ozer}

The \ffmpeg{} command that was used to produce the renditions is similar to the following:

\begin{minted}[frame=single,fontsize=\small]{bash}
ffmpeg -i $INPUT \
  -c:v libx264 -pix_fmt yuv420p -preset veryfast -r 25 \
  -g 50 -keyint_min 50 -sc_threshold 0 \
  -force_key_frames 'expr:gte(t,n_forced*2)' -refs 1 \
  -c:a libfdk_aac -ac 2 -b:a 128k \
  -map 0:a:0 -map 0:v:0 -map 0:v:0 -map 0:v:0 -map 0:v:0 \
  -s:v:0 1280x720 -b:v:0 3500k -bufsize:v:0 3500k -minrate:v:0 3500k -maxrate:v:0 3500k \
  -s:v:1 960x540 -b:v:1 2500k -bufsize:v:1 2500k -minrate:v:1 2500k -maxrate:v:1 2500k \
  -s:v:2 640x360 -b:v:2 1500k -bufsize:v:2 1500k -minrate:v:2 1500k -maxrate:v:2 1500k \
  -s:v:3 480x270 -b:v:3 800k -bufsize:v:3 800k -minrate:v:3 800k -maxrate:v:3 800k \
  abr.mp4
\end{minted}

In detail, the meaning of the options is as follows:

\begin{itemize}
    \item \texttt{-i} specifies the input media file, which can be in any format supported by \ffmpeg{};
    \item \texttt{-c:v} specifies the video codec, in this case \texttt{libx264}, the library that implements x264, a popular open source software-based H.264 encoder;
    \item \texttt{-pix\_fmt} defines the pixel format. A value of \texttt{yuv420p} means that the pixels are encoded with Y'CbCr 4:2:0 chroma subsampling;
    \item \texttt{-preset} specifies a set of configuration options that influence the encoding speed. Research has shown that \texttt{veryfast} is a good trade-off between speed and quality;\cite{ozer}
    \item \texttt{-r} sets the \textit{frames per second} (fps) value, in this case 25;
    \item \texttt{-g} and \texttt{-keyint\_min} tell the encoder the maximum and minimum GOP size in frames, that is, the minimum and maximum interval between key frames (I-frames);
    \item \texttt{-sc\_threshold} adjusts the scene cut detection, an algorithm that analyzes the difference between frames to determine whether a new I-frame should be inserted. Setting the parameter to 0 means that the scene cuts detection is disabled. This is a common choice when encoding for streaming, since I-frames are very expensive compared to other frame types.\cite{ozer}
    \item \texttt{-force\_key\_frames} forces a key frame when the given expression evaluates to \texttt{true} at the current frame. This option is required because \texttt{x264} does not allow setting \texttt{-keyint\_min} to a value greater than \texttt{keyint/2+1}, and clips the value if it is larger.\footnote{\url{https://github.com/mirror/x264/blob/b093bbe7d9bc642c8f24067cbdcc73bb43562eab/encoder/encoder.c\#L1111}} The expression \texttt{gte(t,n\_forced*2)} means that a keyframe should be forced every 2 seconds, since \texttt{t} is the time of the current frame and \texttt{n\_forced} is the number of forced frames;
    \item \texttt{-refs} specifies the number of reference frames that each inter-predicted frame can use. Research has shown that limiting the number of frames to 1 has a negligible impact on quality while reducing the encoding time;\cite{ozer}
    \item \texttt{-c:a} specifies the audio codec, in this case \texttt{libfdk\_aac}, a high quality AAC implementation by the Fraunhofer research institute.
    \item \texttt{-ac} specifies the number of audio channels, 2 in this case;
    \item \texttt{-b:a} sets the audio bitrate, which is constant (CBR) by default;
    \item \texttt{-map} is used to select which streams of the input file should be sent to the output. For example, \texttt{-map 0:a:0} takes the first audio track from the first input file and includes it in the output file. Multiple \texttt{-map} options for the same input stream have the effect of duplicating the stream. In this case, we use \texttt{-map} to create multiple video streams (one per rendition) with a single command;
    \item \texttt{-s:v} is used to scale the video to the specified resolution. In our command this option is used once per rendition to change the resolution for reach of the (duplicated) video streams;
    \item \texttt{-b:v}, \texttt{-minrate:v}, \texttt{-maxrate:v}, \texttt{-bufsize:v} set the average target bitrate, the minimum bitrate, the maximum bitrate, and the buffer size of the encoder. By setting all the options to the same value the result is a constant bitrate (CBR) video stream, which is often recommended for adaptive bitrate streaming and helps to comply with Apple HLS Authoring Specification;\cite{ozer}\cite{hlsauthoring}
\end{itemize}

The output is a single MP4 file containing multiple tracks, one per each video rendition and one for the audio, as shown in Figure \ref{fig:ffprobe_abr}.

\begin{figure}
    \centering
    \begin{minted}[frame=single,breaklines,fontsize=\footnotesize]{text}
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'abr.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    title           : Big Buck Bunny, Sunflower version
    artist          : Blender Foundation 2008, Janus Bager Kristensen 2013
    composer        : Sacha Goedegebure
    encoder         : Lavf59.31.100
    comment         : Creative Commons Attribution 3.0 - http://bbb3d.renderfarming.net
    genre           : Animation
  Duration: 00:10:34.64, start: 0.000000, bitrate: 7884 kb/s
  
  Stream #0:0[0x1](und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s (default)
  
  Stream #0:1[0x2](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 1280x720 [SAR 1:1 DAR 16:9], 3238 kb/s, 25 fps, 25 tbr, 12800 tbn (default)
  
  Stream #0:2[0x3](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 960x540 [SAR 1:1 DAR 16:9], 2341 kb/s, 25 fps, 25 tbr, 12800 tbn (default)
  
  Stream #0:3[0x4](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 640x360 [SAR 1:1 DAR 16:9], 1409 kb/s, 25 fps, 25 tbr, 12800 tbn (default)
  
  Stream #0:4[0x5](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 480x270 [SAR 1:1 DAR 16:9], 751 kb/s, 25 fps, 25 tbr, 12800 tbn (default)
  
    \end{minted}
    \caption{\texttt{ffprobe}'s output showing the 5 video tracks of the ABR MP4 file. Some metadata was omitted for brevity.}
    \label{fig:ffprobe_abr}
\end{figure}

As we have seen, when the emulation starts one of the applications that is run is the live source generation, or ABR packaging. This is again a \ffmpeg{} script, run inside a Docker container. The script takes the previously generated MP4 file as input and outputs \texttt{fMP4} chunks, HLS playlists, and the DASH manifest, for all the renditions.

The \ffmpeg{} command used for packaging is similar to the following:

\begin{minted}[frame=single]{bash}
ffmpeg -re -i $SOURCE \
  -map 0 -c copy \
  -utc_timing_url 'https://time.akamai.com/?iso' \
  -seg_duration 2 \
  -dash_segment_type mp4 \
  -use_template 1 -use_timeline 0 \
  -init_seg_name 'init-stream-$RepresentationID$.m4s' \
  -media_seg_name 'chunk-stream-$RepresentationID$-$Number%05d$.m4s' \
  -adaptation_sets 'id=0,streams=v id=1,streams=a' \
  -hls_playlist 1 \
  -f dash \
  $OUT_DIR/manifest.mpd
\end{minted}

Specifically, the meaning of the arguments and options is as follows.

\begin{itemize}
    \item \texttt{-re} limits the read rate so that it respects the frame rate, otherwise \ffmpeg{} would read the whole video without respecting the playback rate;
    \item \texttt{-c} with a value of \texttt{copy} tells \ffmpeg{} to copy the input streams without re-encoding, since we have already encoded them previously;
    \item \texttt{-utc\_timing\_url} defines the URL of a web page that returns the current UTC timestamp in ISO format. This is needed by DASH on the client side for time synchronization when performing live streaming;
    \item \texttt{-seg\_duration} specifies the duration of each segment or fragment that is generated. For better performance, segments should start with a key frame, which is the reason why we set the value to 2 seconds, corresponding to the GOP length of the input file;
    \item \texttt{-dash\_segment\_size} with a value of \texttt{mp4} specifies that the packer should output CMAF-compliant \texttt{fMP4} segments;
    \item \texttt{-use\_template} and \texttt{-use\_timeline} enable the use of DASH \texttt{SegmentTemplate} to avoid listing all the video segments in the manifest. This is possible because the segments have a fixed duration;
    \item \texttt{-init\_seg\_name} and \texttt{-media\_seg\_name} define the file name format for the initialization segment and the individual media segments;
    \item \texttt{-adaptation\_sets} specifies which adaption sets should be added to the DASH manifest. The expression \texttt{id=0,streams=v id=1,streams=a} defines to adaption sets, one with all the video streams (ID 0) and the other one 
    with the audio stream (ID 1);
    \item \texttt{-hls\_playlist 1} specifies that the DASH muxer should also generate HLS playlists while using the same \texttt{fMP4} segments. The packager will create a \texttt{master.m3u8} file with the master playlist and a set of media playlists with file names \texttt{media\_0.m3u8}, \texttt{media\_1.m3u8}, etc.;
    \item \texttt{-f dash} sets the \ffmpeg{} output muxer to DASH;
    \item finally, the path of the DASH manifest file (\texttt{.mpd}) is specified.
\end{itemize}

The output of the packager, that is the segment files, the DASH manifest and the HLS manifest, are put in a Docker mounted directory which is shared with the CDN container.

\subsection{CDN server}
\label{sec:eval/testbed/cdn}

The "CDN" part of the setup consists in the \texttt{h2o} HTTP server, an open source software maintained by Fastly developers. \texttt{h2o} is built from source during the initial creation of the Docker image, since we are using a fork of \texttt{h2o} with some modifications.

The web server is configured to listen on three separate ports for HTTP/1.1, HTTP/2, and HTTP/3, thus allowing to test the three protocols independently. As we shall see, instead of relying on the \texttt{Alt-Svc} HTTP header to inform the browser about the availability of HTTP/3 on a specific port, we will configure the browser to force HTTP/3 on the corresponding port from the beginning, avoiding upgrades from other protocols.

A problem we encountered was the inability to tell \texttt{h2o} to enable/disable HTTP/2 only on specific ports. Since HTTP/2 only works over TLS, the workaround was to use port 80 for HTTP/1.1 and HTTP/2 on port 443. This works because the only possible option for port 80 is HTTP/1.1, while for port 443 there is a TLS feature, namely Application-Layer Protocol Negotiation (ALPN), that makes sure that browsers send the requests as HTTP/2 and not HTTP/1.1 (given that the server says so during the TLS handshake). HTTP/3 must instead be enabled explicitly, in our case on port 444. Figure \ref{fig:h2o} shows a configuration file for \texttt{h2o} that implements what we have just said.

In order for HTTP/2 and HTTP/3 to work, a public key certificate must be generated and configured on the server side so that a TLS connection can be established. The certificates were generated with the \texttt{mkcert} tool that creates a Certificate Authority (CA) and also a domain ceritifcate signed by the root CA. This is done before starting the HTTP server. The tool makes the operation as simple as this:

\begin{minted}[frame=single]{bash}
# Generate and install the root CA certificate
mkcert -install
# Generate a certificate for the domain "cdn.local"
mkcert cdn.local
\end{minted}

The certificates are exposed by the \texttt{h2o} web server so that the client container can download and install/trust the root CA certificate on the local system.

\begin{figure}
    \centering
    \begin{minted}[frame=single,fontsize=\small,style=vs]{ini}
# HTTP/1.1
listen: 80

# HTTP/2
listen:
  port: 443
  ssl: &ssl
    certificate-file: certs/cdn.local.pem
    key-file: certs/cdn.local-key.pem
    minimum-version: TLSv1.2
    cipher-preference: server
    cipher-suite: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA...

# HTTP/3
listen:
  port: 444
  type: quic
  ssl:
    <<: *ssl

hosts:
  "cdn.local":
    paths:
      /:
        file.dir: www
      /certs:
        file.dir: certs

access-log: /dev/stdout
    \end{minted}
    \caption{Example configuration of the \texttt{h2o} HTTP server showing HTTP/1.1 on port 80, HTTP/2 on port 443, and HTTP/3 on port 444. The directory named \texttt{www} is exposed at the domain root, while the certificates are exposed at \texttt{/certs}.}
    \label{fig:h2o}
\end{figure}

\subsubsection{\texttt{h2o} patches for better priorities support}
\label{sec:eval/testbed/cdn/h2o}

Although \texttt{h2o} supports HTTP/3, during the development of the testbed we discovered that the implementation did not include support for priorities as per RFC 9218 (see Section \ref{sec:bg/http3}). Specifically, reprioritization requested through the \texttt{PRIORITY\_UPDATE} HTTP/3 frame was ignored.

After debugging, we found that \texttt{h2o} was still implementing the first draft of the extensible priorities specification, published in 2020. Between drafts, the type and structure of the \texttt{PRIORITY\_UPDATE} frame changed and \texttt{h2o} still expected the old non-RFC-compliant frame format.\footnote{\url{https://www.ietf.org/rfcdiff?url1=draft-ietf-httpbis-priority-01&url2=draft-ietf-httpbis-priority-02&difftype=--html}}

We therefore patched \texttt{h2o} in our fork to properly support RFC 9218, and then submitted the patch as a pull request to the upstream GitHub repository. The pull request was later merged in the master branch of \texttt{h2o} by the maintainer.\footnote{\url{https://github.com/h2o/h2o/pull/3096}}

Another modification that we have made to \texttt{h2o} consists in adding a way to specify the priority of the request through a query string parameter, e.g. \texttt{GET /segment.mp4?priority=2}. Although this is not strictly needed, it helps in a couple of cases when sending requests through JavaScript in the browser. We will see why in more detail in Section TODO.

\subsection{Emulating the network link}
\label{sec:eval/testbed/network}

Between the client and the server there is the network link that acts as the Internet access link. This link is created through the Python code that starts the emulation, found in the \texttt{topology.py} file in the testbed repository, and connects the two switches where the client and server hosts are connected. In ComNetsEmu, links can be created as follows:

\begin{minted}[frame=single]{python}
net.addLink(h1, s1, bw=100, delay='0ms')
net.addLink(s1, s2, bw=100, delay='10ms')
net.addLink(s2, h2, bw=100, delay='0ms')
\end{minted}

In this case, we are creating a link between host 1 and switch 1, between switch 1 and switch 2, and between switch 2 and host 2. We can also specify the network characteristics of the links, for example bandwidth (Mbps), delay or latency (milliseconds), and packet loss (percentage).

Note that these configuration parameters are applied to both sides of the links. So, for example, it is not possible to have asymmetric links with different bandwidth values for upload and download. Moreover, it must be considered that specifying a delay of 10 ms on a link means that the actual latency introduced by the link is 20 ms per direction, corresponding to a \textbf{Round-Trip Time} (RTT) of 40 milliseconds. However, it is possible to choose different parameters for the two interfaces of the link.

The link parameters in ComNetsEmu can be changed after the links are created. Specifically, the \texttt{addLink} method returns an instance of Mininet's \texttt{Link} class (usually a subclass), which exposes methods to modify the link, as shown in the following code snippet.

\begin{minted}[frame=single]{python}
link.intf1.config(bw=bw, delay=delay, loss=loss)
link.intf2.config(bw=bw, delay=delay, loss=loss)
\end{minted}

To emulate a realistic network with oscillating bandwidth and jittery latency, a way to apply a \textbf{custom network pattern} was needed. This is not something that Mininet or ComNetsEmu provide out of the box, so we implemented a strategy that consists in varying the network configuration of the link interfaces every second of the emulation, depending on a network pattern dataset contained in a CSV file.

This periodic update is actually triggered by the Node.js backend application that is hosted on host 1, but the actual update of the network link configuration must be done through the ComNetsEmu Python code. For this reason, we implemented an \textbf{HTTP API server} in the ComNetsEmu applcation so that the containers can call the API to interact with the emulation "brain".

In practice, there are two endpoints exposed on HTTP port 8080 by the main application:

\begin{itemize}
    \item \texttt{POST /update}, with a JSON payload containing the new parameters of the link between switch 1 and switch 2. In particular, the parameters are \texttt{bw} (in Mbps), \texttt{rtt} (in milliseconds), \texttt{loss} (in percentage).
    \item \texttt{POST /stop}, which stops the emulation and all the containers. The ComNetsEmu application does not know when the experiments are complete, so the experiment runner can communicate this through the stop endpoint.
\end{itemize}

\subsubsection{Network patterns}
\label{sec:eval/testbed/network/patterns}

Two main network datasets were used for the emulations. In particular:

\begin{itemize}
    \item A 4G LTE dataset captured in real-life scenarios by the \textit{Mobile and Internet Systems Laboratory} of the University College Cork.\cite{dataset1};
    \item A synthetic set of network patterns used in the context of the \textit{2020 Grand Challenge on Adaption Algorithms for Near-Second Latency} organized by Twitch for the \textit{ACM Multimedia Systems Conference}.\footnote{\url{https://2020.acmmmsys.org/lll_challenge.php}}
\end{itemize}

The 4G dataset contains traces collected between 2017 and 2018 from two major Irish mobile operators with different mobility patterns (static, pedestrian, car, tram and train). The dataset contains 135 traces with an average duration of 15 minutes and a throughput ranging from 0 to 173 Mbps. The traces are organized in CSV files, and each of them contains general information such as the timestamp, coordinates, and speed, technical information about the mobile link such as the type of network (LTE, HSPA+, etc.), SNR and RSRP, and finally the measured throughput for both downlink and uplink. From this dataset we extracted some 1-minute sequences. % TODO: was it train? How many sequences?

The patterns from Twitch's Grand Challenge are useful when testing low-latency scenarios, since the patterns are purposely designed to hinder adaption algorithms. Two patterns used in our testbed were inspired from this source, specifically the cascade pattern, where the bandwidth is slowly reduced with time, and the spike pattern, where the bandwidth drops abruptly for a fixed period of time.

\subsection{Client backend and headless browser}
\label{sec:eval/testbed/backend}

On the "client" side of the network link, there is host \texttt{H1}, which is responsible for launching a browser instance and playing the video stream in a web page, while collecting useful metrics.

For the browser, we chose to use the Chromium web browser, on which Google Chrome is based. One of the reasons for this choice is the underflow behavior when using unmuxed video and audio tracks, as we will see in Section X, and the support for WebCodecs API.

Chromium is installed when building the Docker image using the default Debian 11 (bullseye) repository, which includes an ARM64 build of Chromium. However, by default, Chromium only ships with non-proprietary codecs support, such as VP9 and AV1, and not H.264.\footnote{\url{https://www.chromium.org/audio-video/}} To be able to play H.264 video files, an additional package must be installed. This package (\texttt{chromium-codecs-ffmpeg-extra}) is not provided by Debian repositories, so the (equivalent) Ubuntu version is used.

Before starting Chromium, the root Certificate Authority certificate must be trusted by the system (within the container), so that the player can download the manifest files and segments without incurring certificate errors. This can be done at each run by downloading the root CA certificate from the CDN server and then installing it system-wide locally with a tool like \texttt{certutil}. Chromium will use the system certificates database and trust the local CDN domain as if the certificate was not actually self-signed.

The Chromium browser is launched as a headless\footnote{Without the GUI.} instance through Google's \textbf{Puppeteer}, a popular Node.js module to control Chromium through code. For this reason, the backend application was written with TypeScript and is based on Node.js.\footnote{\url{https://pptr.dev/}}

There are a few configuration parameters of Puppeteer that are important for our setup:

\begin{itemize}
    \item Since we already installed the Chromium browser, we must tell Puppeteer not to download it at startup. This can be done with the environment variable \texttt{PUPPETEER\_SKIP\_CHROMIUM\_DOWNLOAD} set to \texttt{true}. We can then specify where Puppeteer should find the Chromium executable with \texttt{PUPPETEER\_EXECUTABLE\_PATH} (e.g. \texttt{/usr/bin/chromium});
    \item When launching Chromium through Puppeteer, we must specify some command line options so that QUIC is used by default on port 444, as we have seen above. In practice, this can be done with the options \texttt{--enable-quic} and \texttt{--origin-to-force-quic-on=cdn.local:444}.
\end{itemize}

Note that specifically trusting the certificate that was generated with the custom Certificate Authority is not required since Chromium is already trusting the root CA certificate and thus all the certificates with the CA in the chain.

An alternative way to have Chromium trust the certificate, which can be useful when developing and testing outside the testbed setup, is to use the \texttt{--ignore-certificate-errors-spki-list} option to specify the base64-encoded SHA-256 hashes (or SPKI fingerprints) of the public key certificates that must be trusted.

The fingerprint for a certificate can be found with OpenSSL like this:

\begin{minted}[frame=single]{bash}
openssl x509 -noout -pubkey -in cdn.local.pem |
  openssl pkey -pubin -outform der |
  openssl dgst -sha256 -binary |
  base64
\end{minted}

An example command that launches Google Chrome on macOS by trusting a specific certificate is the following:

\begin{minted}[frame=single,breaklines]{bash}
open -a "Google Chrome" --args --enable-quic --origin-to-force-quic-on=cdn.local:444 --ignore-certificate-errors-spki-list=PzvKkGfTAvrQWXHpEnmXssTywk7rHhscPwokTCMqtyg=
\end{minted}

After launching Chromium, the backend application navigates to the frontend UI, which is exposed on localhost. It then starts the emulation by clicking a button.

The backend also takes care of communicating with the frontend, coordinating the emulation, and collecting and storing the player metrics for subsequent analysis. Communication with the frontend is done through a \texttt{tRPC} API and WebSockets. \texttt{tRPC} is a Node.js library for building typesafe APIs with TypeScript. The backend defines the queries (read operations) and mutations (write operations) that are exposed to the frontend. The frontend then uses the \texttt{tRPC} library to access the endpoints.

Several \texttt{tRPC} mutations were defined, corresponding to events that the player captures and transmits. We will see these events in more detail in Section \ref{sec:eval/testbed/metrics}. When the start button is clicked by Puppeteer at the start of the emulation, the frontend calls the \texttt{startExperiments} RPC which loops through the set of experiments and runs them in sequence.

In a couple of cases the emulation requires bi-direction communication with the frontend. An example is the backend requesting the frontend to reset the player to start a new playback session, or to actually start the new session with a given set of parameters.

At the time of writing, \texttt{tRPC}'s support for subscriptions based on WebSockets was incomplete. Therefore, we decided to implement a custom JSON-based protocol over WebSockets. Each JSON message contains the field \texttt{type}, plus other optional fields.

The two main types of commands transmitted by the backed to the frontend are:

\begin{itemize}
    \item \texttt{reset}, which has the effect of destroying the player and stoping metrics collection;
    \item \texttt{start}, which starts a new playback session with the following parameters:
    \begin{itemize}
        \item the ABR protocol (HLS or DASH);
        \item the URL of the DASH manifest or HLS master playlist. This parameter determines which HTTP version is used, since different HTTP protocols are enabled on the different ports;
        \item the minimum bitrate to use in ABR;
        \item whether the live catchup feature of players should be enabled.
    \end{itemize}
\end{itemize}

As previously mentioned, experiments that are defined in the backend are executed sequentially when the emulation starts. Each experiment is defined by the \texttt{Experiment} class, which can be subclassed as needed to implement more specific use cases. The default implementation allows to set the following configuration parameters:

\begin{itemize}
    \item The name of the experiment, which is then used to compose the name of the output file name that will contain the collected data;
    \item the name of the network pattern to use, which is read from a CSV file. The CSV file contains one row for each second in which the emulation will run. Each row specifies the download bandwidth, the upload bandwidth, the RTT and the packet loss;
    \item the ABR protocol to use for the experiment (HLS or DASH);
    \item the HTTP version, among HTTP/1.1, HTTP/2, and HTTP/3;
    \item the minimum bitrate;
    \item whether the live catchup feature is enabled.
\end{itemize}


When running an experiment with the default \texttt{Experiment} implementation, a fixed sequence of operations is executed for each experiment. The duration of the experiment however depends on the number of rows contained in the network pattern file. More in detail, the operations that are performed are as follows.

\begin{itemize}
    \item Read the network pattern from the CSV file and parse it;
    \item reset the timer (used to assign a timestamp to each event) and the list of collected events;
    \item send the \texttt{start} command to the frontend so that it can configure the player and start the playback;
    \item loop through the data points of the network pattern and apply the updated network link config once a second. The update is done by calling the ComNetsEmu application API at the \texttt{/update} endpoint, as we have seen in Section \ref{sec:eval/testbed/network}. The hostname of the API is provided through an environment variable;
    \item at the end of the experiment, the player is reset and the list of collected events is serialized and saved to a JSON file.
\end{itemize}

\subsection{Collected metrics and events}
\label{sec:eval/testbed/metrics}



% TODO: how the network patterns CSV files are created

% TODO: frontend typescript. maybe below

% TODO: topopology code



% experiments

% plots

